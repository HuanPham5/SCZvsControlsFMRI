{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-collapsed CNN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc65d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all physical devices of type 'GPU'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f'Number of GPUs available: {len(gpus)}')\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f'GPU {i}: {gpu}')\n",
    "else:\n",
    "    print('No GPU detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2220a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full list of all schizophrenia IDs from csv, 86 in total but not all satisfy have t>90\n",
    "\n",
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c771fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3d_cnn(input_shape, num_classes=1):\n",
    "    model = Sequential([\n",
    "        Conv3D(64, kernel_size=(10, 10, 10), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(128, kernel_size=(5, 5, 5), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(5000, activation='relu'),\n",
    "        Dropout(0.3),  \n",
    "        Dense(5000, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(5000, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (91, 91, 146, 1)\n",
    "model = build_3d_cnn(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, new_shape):\n",
    "    factors = (\n",
    "        new_shape[0]/image.shape[0],\n",
    "        new_shape[1]/image.shape[1],\n",
    "        new_shape[2]/image.shape[2]\n",
    "    )\n",
    "    return scipy.ndimage.zoom(image, factors, order=1)  # order=1 is bilinear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_accuracy = 0.0\n",
    "total_loss = 0.0\n",
    "num_iterations =15\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    ''' data training for GAN '''\n",
    "    ''' Choosing 50 random schizphrenia samples and store remaining 9 test'''\n",
    "    ''' Since we are training a separate GAN for control, also choose 50 random control samples and store remaining 9 test '''\n",
    "\n",
    "    # GAN Training Data Selection\n",
    "    gan_train_ids_schiz = random.sample(met_requirement_schizophrenia_ids, 50)\n",
    "    gan_test_ids_schiz = [id for id in met_requirement_schizophrenia_ids if id not in gan_train_ids_schiz]\n",
    "\n",
    "    gan_train_ids_control = random.sample(met_requirement_control_ids, 50)\n",
    "    gan_test_ids_control = [id for id in met_requirement_control_ids if id not in gan_train_ids_control]\n",
    "\n",
    "    # since we have 82 - 50 = 32, choose 9 random for testing    \n",
    "    gan_test_ids_control = random.sample(gan_test_ids_control,9)\n",
    "\n",
    "    ''' data training for classifier '''\n",
    "    ''' just use the same train set as GAN above '''\n",
    "\n",
    "    # Classifier Training Data Selection\n",
    "    classifier_train_ids = gan_train_ids_control + gan_train_ids_schiz\n",
    "\n",
    "    # Classifier Test Data Selection\n",
    "    classifier_test_ids = gan_test_ids_schiz + gan_test_ids_control\n",
    "\n",
    "    ''' File loading '''\n",
    "    # Specify the directory and file pattern\n",
    "    directory_path = '../4D/'\n",
    "    file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "    # Construct the full path pattern\n",
    "    path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "    # Use glob to find all matching files\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "\n",
    "    ''' File loading for GAN Training and classifer '''\n",
    "    ''' But this time we have 2 separate GANs, 1 train on schizoprenia and 1 train on control'''\n",
    "    \n",
    "    classifier_image_data = []\n",
    "    classifier_labels = []  # 1 for schizophrenia, 0 for non-schizophrenia\n",
    "    gan_image_data_schiz = []\n",
    "    gan_image_data_control = []\n",
    "\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in gan_train_ids_schiz:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0) # 0 for x axis\n",
    "            gan_image_data_schiz.append(t1_data_collapsed)\n",
    "\n",
    "        if file_id in gan_train_ids_control:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0)\n",
    "            gan_image_data_control.append(t1_data_collapsed)\n",
    "\n",
    "        if file_id in classifier_train_ids or file_id in classifier_test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "\n",
    "            # collapsed x-axis\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0)\n",
    "            classifier_image_data.append(t1_data_collapsed)\n",
    "            classifier_labels.append(label)\n",
    "\n",
    "    print(f\"Total GAN training files processed: {len(gan_image_data_control+gan_image_data_schiz)}\")\n",
    "    print(f\"Total classifier training/testing files processed: {len(classifier_image_data)}\")\n",
    "    print(f\"Total labels processed: {len(classifier_labels)}\")\n",
    "  \n",
    "\n",
    "    '''Determine the maximum t-dimension size '''\n",
    "    max_z_size_schiz = max(img.shape[2] for img in gan_image_data_schiz)\n",
    "    max_z_size_control = max(img.shape[2] for img in gan_image_data_control)\n",
    "    \n",
    "\n",
    "    ''' normalization '''\n",
    "    image_data_normalized_schiz = [(img - np.min(img)) / (np.max(img) - np.min(img)) * 2 - 1 for img in gan_image_data_schiz]\n",
    "    image_data_normalized_control = [(img - np.min(img)) / (np.max(img) - np.min(img)) * 2 - 1 for img in gan_image_data_control]\n",
    "    \n",
    "\n",
    "    ''' padding of images data '''\n",
    "    # Pad each image to have a consistent z-dimension size\n",
    "    padded_data_schiz = [np.pad(img, ((0, 0), (0, 0), (0, max_z_size_schiz - img.shape[2])), mode='constant') for img in image_data_normalized_schiz]\n",
    "    padded_data_control = [np.pad(img, ((0, 0), (0, 0), (0, max_z_size_control - img.shape[2])), mode='constant') for img in image_data_normalized_control]\n",
    "\n",
    "    target_size = (91, 91, 146)\n",
    "\n",
    "    # Apply resizing to GAN training images\n",
    "    resized_data_schiz = [resize_image(img, target_size) for img in padded_data_schiz]\n",
    "    resized_data_control = [resize_image(img, target_size) for img in padded_data_control]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Now convert the padded data list to a numpy array\n",
    "    padded_data_array_schiz = np.array(resized_data_schiz)\n",
    "    padded_data_array_control = np.array(resized_data_control)\n",
    "\n",
    "    ''' loading the data for WGAN training '''\n",
    "    train_images_schiz = padded_data_array_schiz\n",
    "    train_images_control = padded_data_array_control\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    train_dataset_schiz = tf.data.Dataset.from_tensor_slices((train_images_schiz)).shuffle(len(train_images_schiz)).batch(batch_size)\n",
    "    train_dataset_control = tf.data.Dataset.from_tensor_slices((train_images_control)).shuffle(len(train_images_control)).batch(batch_size)\n",
    "    \n",
    "    # Create labels for the datasets\n",
    "    labels_schiz = np.ones(len(padded_data_array_schiz))  # 1 for schizophrenia\n",
    "    labels_control = np.zeros(len(padded_data_array_control))  # 0 for control\n",
    "\n",
    "    # Combine the data and labels\n",
    "    combined_images = np.concatenate([padded_data_array_schiz, padded_data_array_control], axis=0)\n",
    "    combined_labels = np.concatenate([labels_schiz, labels_control], axis=0)\n",
    "\n",
    "    # Shuffle the combined dataset to ensure the data is randomly distributed\n",
    "    # It's important for training a model that generalizes well\n",
    "    indices = np.arange(combined_images.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    shuffled_images = combined_images[indices]\n",
    "    shuffled_labels = combined_labels[indices]\n",
    "\n",
    "    # Convert to TensorFlow Dataset, manually split 80/20\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((shuffled_images, shuffled_labels))\n",
    "    train_size = int(0.8 * len(shuffled_images))\n",
    "    train_dataset = dataset.take(train_size).shuffle(buffer_size=train_size).batch(10)\n",
    "    test_dataset = dataset.skip(train_size).batch(10)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=30,\n",
    "        validation_data=test_dataset\n",
    "    )\n",
    "    # Evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(test_dataset)\n",
    "    print(f\"Training accuracy: {train_acc}, Training loss: {train_loss}\")\n",
    "\n",
    "\n",
    "    ''' load test images to make test set'''\n",
    "    # Now proceed with loading and preprocessing the images for these IDs\n",
    "    test_image_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    test_ids = classifier_test_ids\n",
    "\n",
    "    # Loop through the matching files and filter based on test IDs\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "\n",
    "        # Process only if the ID is in the test set\n",
    "        if file_id in test_ids:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "\n",
    "            # Ensure sufficient time dimension\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Collapse one of the axes by summing\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0)\n",
    "\n",
    "            # Resize, normalize, and add dimension as done in the training data preparation\n",
    "            processed_image = resize_image(t1_data_collapsed, (91, 91, 146))\n",
    "            processed_image_normalized = (processed_image - np.min(processed_image)) / (np.max(processed_image) - np.min(processed_image)) * 2 - 1\n",
    "            processed_image_final = np.expand_dims(processed_image_normalized, axis=-1)\n",
    "\n",
    "            test_image_data.append(processed_image_final)\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "            test_labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    test_images_array = np.array(test_image_data)\n",
    "    test_labels_array = np.array(test_labels)\n",
    "\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_dataset)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "    \n",
    "    total_loss += loss\n",
    "    total_accuracy += accuracy\n",
    "    \n",
    "    predicted_probs = model.predict(test_dataset)\n",
    "    predicted_labels = np.where(predicted_probs > 0.5, 1, 0)  # Convert probabilities to binary labels\n",
    "\n",
    "    # Extract the actual labels from the test dataset\n",
    "    actual_labels = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "\n",
    "    # Print the predicted labels vs actual labels\n",
    "    print(\"Predicted Labels vs Actual Labels:\")\n",
    "    for pred, actual in zip(predicted_labels, actual_labels):\n",
    "        print(f\"Predicted: {pred}, Actual: {actual}\")\n",
    "\n",
    "    # Generate a confusion matrix\n",
    "    cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Calculate and print the average loss and accuracy over all iterations\n",
    "average_test_loss = total_loss / num_iterations\n",
    "average_test_accuracy = total_accuracy / num_iterations\n",
    "print(f\"\\nAverage Test Loss over {num_iterations} iterations: {average_test_loss}\")\n",
    "print(f\"Average Test Accuracy over {num_iterations} iterations: {average_test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b815ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96a4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
