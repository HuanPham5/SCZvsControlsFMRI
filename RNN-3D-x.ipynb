{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 3D: x-collapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, Bidirectional, AdditiveAttention, LayerNormalization\n",
    "from functools import partial\n",
    "from tensorflow.keras import models, layers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and pad the data\n",
    "def normalize_and_pad(data, max_t):\n",
    "    # Normalize data to range [-1, 1]\n",
    "    normalized = (data - np.min(data)) / (np.max(data) - np.min(data)) * 2 - 1\n",
    "    # Pad the time dimension to max_t\n",
    "    padded = np.pad(normalized, ((0, 0), (0, 0), (0, max_t - data.shape[2])), mode='constant')\n",
    "    return padded\n",
    "\n",
    "# Define a generator function to yield data batches\n",
    "def data_generator(images, labels, batch_size):\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        \n",
    "        # Convert list of arrays to numpy array and add channel dimension\n",
    "        # New shape should be (batch_size, time_steps, y, z, 1)\n",
    "        batch_images = np.array(batch_images).transpose(0, 3, 1, 2)[..., np.newaxis]  # Adjust axes after x-axis collapse\n",
    "        yield batch_images, np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "test_loss_list = []\n",
    "test_accuracy_list = []\n",
    "\n",
    "# Run 15 iterations\n",
    "for iteration in range(15):\n",
    "    print(f\"\\n--- Iteration {iteration + 1} ---\")\n",
    "\n",
    "    # Training Data Selection\n",
    "    train_ids_schiz = random.sample(met_requirement_schizophrenia_ids, 50)\n",
    "    test_ids_schiz = [id for id in met_requirement_schizophrenia_ids if id not in train_ids_schiz]\n",
    "\n",
    "    train_ids_control = random.sample(met_requirement_control_ids, 50)\n",
    "    test_ids_control = [id for id in met_requirement_control_ids if id not in train_ids_control]\n",
    "    test_ids_control = random.sample(test_ids_control, 9)\n",
    "\n",
    "    # Classifier Test Data Selection\n",
    "    classifier_test_ids = test_ids_schiz + test_ids_control\n",
    "\n",
    "    # Specify the directory and file pattern\n",
    "    directory_path = '../4D/'\n",
    "    file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "    # Construct the full path pattern\n",
    "    path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "    # Use glob to find all matching files\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "\n",
    "    # Lists to store data for training and classification\n",
    "    image_data_schiz = []\n",
    "    image_data_control = []\n",
    "\n",
    "    # Iterate through matching files and load data\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in train_ids_schiz or file_id in train_ids_control:\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "            \n",
    "            # Filter out data with insufficient time points\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Collapse the x-axis by summing over it\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0)  # Sum over x-axis\n",
    "\n",
    "            # Store collapsed data in the appropriate list\n",
    "            if file_id in train_ids_schiz:\n",
    "                image_data_schiz.append(t1_data_collapsed)\n",
    "            elif file_id in train_ids_control:\n",
    "                image_data_control.append(t1_data_collapsed)\n",
    "\n",
    "    # Display total loaded data counts\n",
    "    print(f\"Total control loaded: {len(image_data_control)}\")\n",
    "    print(f\"Total schiz loaded: {len(image_data_schiz)}\")\n",
    "\n",
    "    # Determine the maximum time-dimension size\n",
    "    max_t_size_schiz = max(img.shape[2] for img in image_data_schiz)  # Time dimension is now at index 2 after collapsing\n",
    "    max_t_size_control = max(img.shape[2] for img in image_data_control)\n",
    "    max_t_size = max(max_t_size_schiz, max_t_size_control)\n",
    "\n",
    "    # Apply normalization and padding to all loaded data\n",
    "    padded_data_schiz = [normalize_and_pad(img, max_t_size) for img in image_data_schiz]\n",
    "    padded_data_control = [normalize_and_pad(img, max_t_size) for img in image_data_control]\n",
    "\n",
    "    # Convert lists to numpy arrays for model input\n",
    "    padded_data_array_schiz = np.array(padded_data_schiz)\n",
    "    padded_data_array_control = np.array(padded_data_control)\n",
    "\n",
    "    # Print the shape of the padded arrays\n",
    "    print(f\"Shape after normalization and padding (control): {padded_data_array_control[0].shape}\")\n",
    "    print(f\"Shape after normalization and padding (schiz): {padded_data_array_schiz[0].shape}\")\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    # Create labels\n",
    "    labels_schiz = np.ones(len(padded_data_array_schiz))\n",
    "    labels_control = np.zeros(len(padded_data_array_control))\n",
    "\n",
    "    # Combine images and labels\n",
    "    train_images = np.concatenate((padded_data_array_schiz, padded_data_array_control), axis=0)  # Concatenate along the first dimension\n",
    "    train_labels = np.concatenate((labels_schiz, labels_control), axis=0)\n",
    "\n",
    "    # Shuffle indices\n",
    "    indices = np.arange(len(train_images))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Shuffle data based on indices\n",
    "    train_images = [train_images[i] for i in indices]\n",
    "    train_labels = train_labels[indices]\n",
    "\n",
    "    # Create TensorFlow Dataset from the generator\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(train_images, train_labels, batch_size),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, max_t_size, 109, 91, 1), dtype=tf.float32),  # Adjusted shape for ConvLSTM3D after collapsing x-axis\n",
    "            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Prefetch for performance improvement\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Debug: Test the generator\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(f\"Batch image shape: {images.shape}\")  # Should be (batch_size, max_t_size, 109, 91, 1)\n",
    "        print(f\"Batch labels shape: {labels.shape}\")\n",
    "\n",
    "    # Define the RNN model\n",
    "    def build_rnn_model():\n",
    "        # Define input shape: (time_steps, y, z, channels)\n",
    "        time_steps = max_t_size  # Number of time points\n",
    "        input_shape = (time_steps, 109, 91, 1)  # Shape after collapsing x-axis and adding channel\n",
    "\n",
    "        # Input layer\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        convlstm_out = layers.ConvLSTM2D(filters=16, kernel_size=(3, 3), padding='same', return_sequences=True, data_format='channels_last')(inputs)\n",
    "        pooled_out = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling2D())(convlstm_out)\n",
    "        dense_out = tf.keras.layers.Dense(256, activation='relu')(pooled_out)\n",
    "        outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_out)\n",
    "\n",
    "        adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs, outputs)\n",
    "        model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    # Create the model\n",
    "    rnn_model = build_rnn_model()\n",
    "    rnn_model.summary()\n",
    "\n",
    "    # Metrics for training\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "    # Define number of epochs\n",
    "    epochs = 75\n",
    "\n",
    "    # Define a step function for training\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = rnn_model(images, training=True)\n",
    "            # Compute binary cross-entropy loss\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels, predictions[:, -1, 0])\n",
    "\n",
    "        # Apply gradients\n",
    "        gradients = tape.gradient(loss, rnn_model.trainable_variables)\n",
    "        rnn_model.optimizer.apply_gradients(zip(gradients, rnn_model.trainable_variables))\n",
    "\n",
    "        # Update training metrics\n",
    "        train_loss.update_state(loss)\n",
    "        train_accuracy.update_state(labels, predictions[:, -1, 0])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        # Iterate through the training dataset\n",
    "        for images, labels in train_dataset:\n",
    "            train_step(images, labels)\n",
    "\n",
    "        # Print the loss and accuracy for the current epoch\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss.result().numpy()}, Training Accuracy: {train_accuracy.result().numpy()}\")\n",
    "    \n",
    "    # Store training metrics for this iteration\n",
    "    train_loss_list.append(train_loss.result().numpy())\n",
    "    train_accuracy_list.append(train_accuracy.result().numpy())\n",
    "\n",
    "    # List to store test images and labels\n",
    "    test_image_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    # Test IDs to filter relevant files\n",
    "    test_ids = classifier_test_ids\n",
    "\n",
    "    # Iterate through matching files and filter based on test IDs\n",
    "    for file_path in matching_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        file_id = filename.split('_')[0]\n",
    "        \n",
    "        if file_id in test_ids:\n",
    "            # Load the MRI image\n",
    "            t1_img = nib.load(file_path)\n",
    "            t1_data = t1_img.get_fdata()\n",
    "            \n",
    "            # Filter out images with insufficient time points\n",
    "            if t1_data.shape[3] < 90:\n",
    "                continue\n",
    "\n",
    "            # Collapse the x-axis by summing over it\n",
    "            t1_data_collapsed = np.sum(t1_data, axis=0)  # Sum over x-axis (axis=0)\n",
    "            \n",
    "            # Normalize the processed image (shape: (109, 91, t))\n",
    "            processed_image_normalized = (t1_data_collapsed - np.min(t1_data_collapsed)) / (np.max(t1_data_collapsed) - np.min(t1_data_collapsed)) * 2 - 1\n",
    "\n",
    "            # Pad or truncate the time dimension to match the expected size (max_t_size)\n",
    "            current_t_size = processed_image_normalized.shape[2]\n",
    "            if current_t_size < max_t_size:\n",
    "                pad_size = max_t_size - current_t_size\n",
    "                processed_image_padded = np.pad(\n",
    "                    processed_image_normalized, \n",
    "                    ((0, 0), (0, 0), (0, pad_size)), \n",
    "                    mode='constant'\n",
    "                )\n",
    "            elif current_t_size > max_t_size:\n",
    "                processed_image_padded = processed_image_normalized[:, :, :max_t_size]\n",
    "            else:\n",
    "                processed_image_padded = processed_image_normalized\n",
    "\n",
    "            # Reshape to add channel dimension (shape: (109, 91, max_t_size, 1))\n",
    "            processed_image_padded = np.expand_dims(processed_image_padded, axis=-1)  # Add channel dimension\n",
    "\n",
    "            # Transpose to match (time, y, z, channels) for ConvLSTM2D (shape: (max_t_size, 109, 91, 1))\n",
    "            processed_image_transposed = np.transpose(processed_image_padded, (2, 0, 1, 3))\n",
    "\n",
    "            # Append to test image data\n",
    "            test_image_data.append(processed_image_transposed)\n",
    "            \n",
    "            # Determine the label: 1 for schizophrenia, 0 for control\n",
    "            label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "            test_labels.append(label)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier handling in TensorFlow\n",
    "    test_images_array = np.array(test_image_data)\n",
    "    test_labels_array = np.array(test_labels)\n",
    "\n",
    "    # Create a TensorFlow dataset from the numpy arrays\n",
    "    batch_size = 1\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)\n",
    "\n",
    "    # Prefetch for performance improvement\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Debug: Check the shapes of the test data\n",
    "    # for images, labels in test_dataset.take(1):\n",
    "    #     print(f\"Batch image shape: {images.shape}\")  # Expected shape: (batch_size, max_t_size, 109, 91, 1)\n",
    "    #     print(f\"Batch labels shape: {labels.shape}\")  # Expected shape: (batch_size,)\n",
    "\n",
    "    def evaluation_step(images, labels):\n",
    "        # Get predictions from the model\n",
    "        predictions = rnn_model(images, training=False)\n",
    "        \n",
    "        # Compute binary cross-entropy loss\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, predictions[:, -1, 0])\n",
    "        \n",
    "        # Update evaluation metrics\n",
    "        test_loss.update_state(loss)\n",
    "        test_accuracy.update_state(labels, predictions[:, -1, 0])\n",
    "\n",
    "    # Initialize metrics for evaluation\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "    # Reset states before evaluating\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    # Evaluate on test_dataset without using strategy.run()\n",
    "    for images, labels in test_dataset:\n",
    "        evaluation_step(images, labels)\n",
    "\n",
    "    # Store evaluation metrics for this iteration\n",
    "    test_loss_list.append(test_loss.result().numpy())\n",
    "    test_accuracy_list.append(test_accuracy.result().numpy())\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(f\"Test Loss: {test_loss.result().numpy()}, Test Accuracy: {test_accuracy.result().numpy()}\")\n",
    "\n",
    "    # Predict the probabilities on the test dataset\n",
    "    predictions = rnn_model.predict(test_dataset)\n",
    "\n",
    "    # Print shape of predictions for debugging\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "    # Convert probabilities to class labels (binary classification)\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "    # Extract the last time step prediction for each sequence\n",
    "    predicted_labels_last = predicted_labels[:, -1, 0]  # Extract last time step and flatten\n",
    "\n",
    "    # Print the shape of predicted_labels_last for debugging\n",
    "    print(f\"Predicted labels shape (last time step): {predicted_labels_last.shape}\")\n",
    "\n",
    "    actual_labels = test_labels_array\n",
    "    # Extract actual labels for comparison (actual labels are already at the sequence level)\n",
    "    actual_labels_per_sequence = actual_labels.flatten()  # Just flatten, no slicing needed\n",
    "\n",
    "    # Print the shape of extracted actual labels for comparison\n",
    "    print(f\"Actual labels per sequence shape: {actual_labels_per_sequence.shape}\")\n",
    "\n",
    "    # Check if the lengths of actual and predicted labels match before evaluation\n",
    "    if len(actual_labels_per_sequence) == len(predicted_labels_last):\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "        # Compute and print confusion matrix\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(actual_labels_per_sequence, predicted_labels_last))\n",
    "        \n",
    "        # Compute and print classification report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(actual_labels_per_sequence, predicted_labels_last))\n",
    "    else:\n",
    "        print(\"The lengths of actual labels per sequence and predicted labels still don't match.\")\n",
    "        print(f\"Length of actual labels per sequence: {len(actual_labels_per_sequence)}\")\n",
    "        print(f\"Length of predicted labels: {len(predicted_labels_last)}\")\n",
    "\n",
    "    # Create a DataFrame to compare predicted vs. actual labels\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Predicted Labels': predicted_labels_last,\n",
    "        'Actual Labels': actual_labels_per_sequence\n",
    "    })\n",
    "\n",
    "    # Print the full comparison DataFrame\n",
    "    print(comparison_df)\n",
    "\n",
    "average_train_loss = np.mean(train_loss_list)\n",
    "average_train_accuracy = np.mean(train_accuracy_list)\n",
    "average_test_loss = np.mean(test_loss_list)\n",
    "average_test_accuracy = np.mean(test_accuracy_list)\n",
    "\n",
    "# Print average results\n",
    "print(f\"\\nAverage Training Loss over 15 iterations: {average_train_loss}\")\n",
    "print(f\"Average Training Accuracy over 15 iterations: {average_train_accuracy}\")\n",
    "print(f\"Average Test Loss over 15 iterations: {average_test_loss}\")\n",
    "print(f\"Average Test Accuracy over 15 iterations: {average_test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
