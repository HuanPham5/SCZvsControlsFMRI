{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f502c5-7cee-4bbc-afca-d7bd678c3f0e",
   "metadata": {},
   "source": [
    "# Test accuracy RNN-4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ac30aa-54f3-4de8-88e5-0a9c6159d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "from tensorflow.keras.layers import Dropout, Dense, Reshape, Flatten, Conv3D, Conv3DTranspose, LeakyReLU, Input, Embedding, multiply, Concatenate\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization, Bidirectional, AdditiveAttention, LayerNormalization\n",
    "from functools import partial\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b8aed4-93cb-4392-a133-6cc2bace4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_schizophrenia_ids = [\n",
    "    'A00009280', 'A00028806', 'A00023132', 'A00014804', 'A00016859', 'A00021598', 'A00001181', 'A00023158',\n",
    "    'A00024568', 'A00028405', 'A00001251', 'A00000456', 'A00015648', 'A00002405', 'A00027391', 'A00016720',\n",
    "    'A00018434', 'A00016197', 'A00027119', 'A00006754', 'A00009656', 'A00038441', 'A00012767', 'A00034273',\n",
    "    'A00028404', 'A00035485', 'A00024684', 'A00018979', 'A00027537', 'A00004507', 'A00001452', 'A00023246',\n",
    "    'A00027410', 'A00014719', 'A00024510', 'A00000368', 'A00019293', 'A00014830', 'A00015201', 'A00018403',\n",
    "    'A00037854', 'A00024198', 'A00001243', 'A00014590', 'A00002337', 'A00024953', 'A00037224', 'A00027616',\n",
    "    'A00001856', 'A00037619', 'A00024228', 'A00038624', 'A00037034', 'A00037649', 'A00022500', 'A00013216',\n",
    "    'A00020787', 'A00028410', 'A00002480', 'A00028303', 'A00020602', 'A00024959', 'A00018598', 'A00014636',\n",
    "    'A00019349', 'A00017147', 'A00023590', 'A00023750', 'A00031597', 'A00015518', 'A00018317', 'A00016723',\n",
    "    'A00021591', 'A00023243', 'A00017943', 'A00023366', 'A00014607', 'A00020414', 'A00035003', 'A00028805',\n",
    "    'A00029486', 'A00000541', 'A00028408', 'A00000909', 'A00031186', 'A00000838' ]\n",
    "\n",
    "# schizohrenia_id that satisfy t>90, 59 in total\n",
    "met_requirement_schizophrenia_ids = [\n",
    "    'A00000368', 'A00000456', 'A00000541', 'A00000838', 'A00001251', 'A00001452', 'A00004507',\n",
    "    'A00006754', 'A00009280', 'A00012767', 'A00013216', 'A00014607', 'A00014719', 'A00014804',\n",
    "    'A00014830', 'A00015201', 'A00015648', 'A00016197', 'A00016720', 'A00016723', 'A00017147',\n",
    "    'A00018317', 'A00018403', 'A00018434', 'A00018979', 'A00019293', 'A00020414', 'A00020602', \n",
    "    'A00020787', 'A00021591', 'A00021598', 'A00023158', 'A00023246', 'A00023590', 'A00023750', \n",
    "    'A00024198', 'A00024228', 'A00024568', 'A00024684', 'A00024953', 'A00024959', 'A00027410', \n",
    "    'A00027537', 'A00028303', 'A00028404', 'A00028408', 'A00028805', 'A00028806', 'A00031186', \n",
    "    'A00031597', 'A00034273', 'A00035003', 'A00035485', 'A00037034', 'A00037224', 'A00037619', \n",
    "    'A00037649', 'A00038441', 'A00038624']\n",
    "\n",
    "full_control_ids = [\n",
    "    'A00007409', 'A00013140', 'A00021145', 'A00036049', 'A00022810', 'A00002198', 'A00020895', 'A00004667',\n",
    "    'A00015826', 'A00023120', 'A00022837', 'A00010684', 'A00009946', 'A00037318', 'A00033214', 'A00022490',\n",
    "    'A00023848', 'A00029452', 'A00037564', 'A00036555', 'A00023095', 'A00022729', 'A00024955', 'A00024160',\n",
    "    'A00011725', 'A00027487', 'A00024446', 'A00014898', 'A00015759', 'A00028409', 'A00017294', 'A00014522',\n",
    "    'A00012995', 'A00031764', 'A00025969', 'A00033147', 'A00018553', 'A00023143', 'A00036916', 'A00028052',\n",
    "    'A00023337', 'A00023730', 'A00020805', 'A00020984', 'A00000300', 'A00010150', 'A00024932', 'A00035537',\n",
    "    'A00022509', 'A00028406', 'A00004087', 'A00035751', 'A00023800', 'A00027787', 'A00022687', 'A00023866',\n",
    "    'A00021085', 'A00022619', 'A00036897', 'A00019888', 'A00021058', 'A00022835', 'A00037495', 'A00026945',\n",
    "    'A00018716', 'A00026907', 'A00023330', 'A00016199', 'A00037238', 'A00023131', 'A00014120', 'A00021072',\n",
    "    'A00037665', 'A00022400', 'A00003150', 'A00024372', 'A00021081', 'A00022592', 'A00022653', 'A00013816',\n",
    "    'A00014839', 'A00031478', 'A00014225', 'A00013363', 'A00037007', 'A00020968', 'A00024301', 'A00024820',\n",
    "    'A00035469', 'A00029226', 'A00022915', 'A00022773', 'A00024663', 'A00036844', 'A00009207', 'A00024535',\n",
    "    'A00022727', 'A00011265', 'A00024546'\n",
    "]\n",
    "\n",
    " # 82 controls that met requirement\n",
    "met_requirement_control_ids = [\n",
    "    'A00000300', 'A00002198', 'A00003150', 'A00004087', 'A00007409', 'A00010684', 'A00011265', 'A00011725',\n",
    "    'A00012995', 'A00013140', 'A00013816', 'A00014839', 'A00014898', 'A00015759', 'A00015826', 'A00018553',\n",
    "    'A00018716', 'A00019888', 'A00020805', 'A00020895', 'A00020968', 'A00020984', 'A00021058', 'A00021072',\n",
    "    'A00021081', 'A00021085', 'A00022400', 'A00022490', 'A00022509', 'A00022592', 'A00022619', 'A00022653',\n",
    "    'A00022687', 'A00022727', 'A00022729', 'A00022773', 'A00022810', 'A00022835', 'A00022837', 'A00022915',\n",
    "    'A00023095', 'A00023120', 'A00023131', 'A00023143', 'A00023330', 'A00023337', 'A00023730', 'A00023800',\n",
    "    'A00023848', 'A00023866', 'A00024160', 'A00024301', 'A00024372', 'A00024446', 'A00024535', 'A00024546', \n",
    "    'A00024663', 'A00024820', 'A00024932', 'A00024955', 'A00025969', 'A00026945', 'A00027487', 'A00027787', \n",
    "    'A00028052', 'A00028406', 'A00028409', 'A00029226', 'A00029452', 'A00031478', 'A00031764', 'A00033214', \n",
    "    'A00035751', 'A00036049', 'A00036555', 'A00036844', 'A00037007', 'A00037238', 'A00037318', 'A00037495', \n",
    "    'A00037564', 'A00037665'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c3d42-3611-40ed-878d-963a78552171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Selection\n",
    "train_ids_schiz = random.sample(met_requirement_schizophrenia_ids, 50)\n",
    "test_ids_schiz = [id for id in met_requirement_schizophrenia_ids if id not in train_ids_schiz]\n",
    "\n",
    "train_ids_control = random.sample(met_requirement_control_ids, 50)\n",
    "test_ids_control = [id for id in met_requirement_control_ids if id not in train_ids_control]\n",
    "test_ids_control = random.sample(test_ids_control,9)\n",
    "\n",
    "''' data training for classifier '''\n",
    "''' just use the same train set as above '''\n",
    "\n",
    "# Classifier Test Data Selection\n",
    "classifier_test_ids = test_ids_schiz + test_ids_control\n",
    "\n",
    "''' File loading '''\n",
    "# Specify the directory and file pattern\n",
    "directory_path = '4D/'\n",
    "file_pattern = 'A*_????_func_FL_FD_RPI_DSP_MCF_SS_SM_Nui_CS_InStandard.nii.gz'\n",
    "\n",
    "# Construct the full path pattern\n",
    "path_pattern = f'{directory_path}/{file_pattern}'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "matching_files = glob.glob(path_pattern)\n",
    "\n",
    "image_data_schiz = []\n",
    "image_data_control = []\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in train_ids_schiz:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "        \n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        image_data_schiz.append(t1_data)\n",
    "\n",
    "    if file_id in train_ids_control:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "\n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "\n",
    "        image_data_control.append(t1_data)\n",
    "\n",
    "\n",
    "print(f\"Total control loaded: {len(image_data_control)}\")\n",
    "print(f\"Total schiz loaded: {len(image_data_schiz)}\")\n",
    "\n",
    "'''Determine the maximum time-dimension size '''\n",
    "max_z_size_schiz = max(img.shape[3] for img in image_data_schiz)\n",
    "max_z_size_control = max(img.shape[3] for img in image_data_control)\n",
    "max_t_size = max(max_z_size_schiz,max_z_size_control)\n",
    "\n",
    "# Normalize and pad the data\n",
    "def normalize_and_pad(data, max_t):\n",
    "    normalized = (data - np.min(data)) / (np.max(data) - np.min(data)) * 2 - 1\n",
    "    padded = np.pad(normalized, ((0, 0), (0, 0), (0, 0), (0, max_t - data.shape[3])), mode='constant')\n",
    "    return padded\n",
    "\n",
    "padded_data_schiz = [normalize_and_pad(img, max_t_size) for img in image_data_schiz]\n",
    "padded_data_control = [normalize_and_pad(img, max_t_size) for img in image_data_control]\n",
    "\n",
    "padded_data_array_schiz = padded_data_schiz\n",
    "padded_data_array_control = padded_data_control\n",
    "print(\"shape after normalization and padding\", padded_data_array_control[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e60c0e-dc56-4cf0-bc22-808a2f0a21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('RNN-4D.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b2f9b9e-91af-4480-afe0-1a0616bf7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "test_ids = classifier_test_ids  # List of IDs to filter test data\n",
    "\n",
    "for file_path in matching_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    file_id = filename.split('_')[0]\n",
    "    \n",
    "    if file_id in test_ids:\n",
    "        t1_img = nib.load(file_path)\n",
    "        t1_data = t1_img.get_fdata()\n",
    "        \n",
    "        if t1_data.shape[3] < 90:\n",
    "            continue\n",
    "    \n",
    "        # Normalize the processed image\n",
    "        processed_image_normalized = (t1_data - np.min(t1_data)) / (np.max(t1_data) - np.min(t1_data)) * 2 - 1\n",
    "\n",
    "        # Pad or truncate the time dimension to match the expected size (max_t_size)\n",
    "        current_t_size = processed_image_normalized.shape[3]\n",
    "        if current_t_size < max_t_size:\n",
    "            pad_size = max_t_size - current_t_size\n",
    "            processed_image_padded = np.pad(\n",
    "                processed_image_normalized, \n",
    "                ((0, 0), (0, 0), (0, 0), (0, pad_size)), \n",
    "                mode='constant'\n",
    "            )\n",
    "        elif current_t_size > max_t_size:\n",
    "            processed_image_padded = processed_image_normalized[:, :, :, :max_t_size]\n",
    "        else:\n",
    "            processed_image_padded = processed_image_normalized\n",
    "\n",
    "        # Reshape to add channel dimension\n",
    "        processed_image_padded = np.expand_dims(processed_image_padded, axis=-1)  # Shape: (91, 109, 91, t, 1)\n",
    "        processed_image_padded = np.transpose(processed_image_padded, (3, 0, 1, 2, 4))\n",
    "        \n",
    "        test_image_data.append(processed_image_padded)\n",
    "        \n",
    "        label = 1 if file_id in met_requirement_schizophrenia_ids else 0\n",
    "        test_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays for easier handling in TensorFlow\n",
    "test_images_array = np.array(test_image_data)\n",
    "\n",
    "# Reshape each label to match (146, 1) if applicable.\n",
    "reshaped_labels = []\n",
    "for label in test_labels:\n",
    "    # If label corresponds to a full sequence, adjust shape to (146, 1).\n",
    "    reshaped_labels.append(np.full((146, 1), label))\n",
    "\n",
    "# Convert the reshaped labels list into a numpy array.\n",
    "test_labels_array = np.array(reshaped_labels)\n",
    "\n",
    "# Create a TensorFlow dataset from the numpy arrays\n",
    "batch_size = 1\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_array, test_labels_array)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f862cdf-0ad0-4755-83dd-09392eb2c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics for evaluation\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')\n",
    "\n",
    "# Reset states before evaluating\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "\n",
    "# Define evaluation step without using tf.function\n",
    "def evaluation_step(images, labels):\n",
    "    predictions = loaded_model(images, training=False)\n",
    "    loss = tf.keras.losses.binary_crossentropy(labels, predictions[:, -1, 0])\n",
    "    test_loss.update_state(loss)\n",
    "    test_accuracy.update_state(labels, predictions[:, -1, 0])\n",
    "\n",
    "# Evaluate on test_dataset\n",
    "for images, labels in test_dataset:\n",
    "    evaluation_step(images, labels)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Test Loss: {test_loss.result().numpy()}, Test Accuracy: {test_accuracy.result().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270edcc-8bca-4951-9879-317d90d6c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_labels_array contains your actual labels\n",
    "actual_labels = test_labels_array\n",
    "\n",
    "# Predict the probabilities\n",
    "predictions = loaded_model.predict(test_dataset)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Extract the last time step prediction for each sequence\n",
    "predicted_labels_last = predicted_labels[:, -1].flatten()\n",
    "\n",
    "# Flatten the actual labels to match the shape\n",
    "actual_labels_flat = actual_labels.flatten()\n",
    "\n",
    "# Extract one label per sequence from actual labels to match predicted labels\n",
    "actual_labels_per_sequence = actual_labels_flat[::146]  # Assuming each sequence has 146 time steps\n",
    "\n",
    "# Check if the lengths match and print the confusion matrix and classification report\n",
    "if len(actual_labels_per_sequence) == len(predicted_labels_last):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    print(confusion_matrix(actual_labels_per_sequence, predicted_labels_last))\n",
    "    print(classification_report(actual_labels_per_sequence, predicted_labels_last))\n",
    "else:\n",
    "    print(\"The lengths of actual labels per sequence and predicted labels still don't match.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551d5d0a-a925-4705-ac9e-a8ff7d99a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to compare predicted vs. actual labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Predicted Labels': predicted_labels_last,\n",
    "    'Actual Labels': actual_labels_per_sequence\n",
    "})\n",
    "\n",
    "# Display the first few rows of the comparison\n",
    "print(comparison_df.head())\n",
    "\n",
    "# Print the full comparison DataFrame\n",
    "print(comparison_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
